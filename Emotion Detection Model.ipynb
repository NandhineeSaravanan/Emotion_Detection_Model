{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d79ea6",
   "metadata": {},
   "source": [
    "# Emotion Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b5ae5",
   "metadata": {},
   "source": [
    "Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d623d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from string import punctuation \n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re #regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816dae3",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9573b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use pandas to load our dataset\n",
    "df=pd.read_csv('emotion-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5914b3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral                                             Why ? \n",
       "1      joy    Sage Act upgrade on my to do list for tommorow.\n",
       "2  sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...\n",
       "3      joy   Such an eye ! The true hazel eye-and so brill...\n",
       "4      joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the structure of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e902dd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34792, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a08088f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion    0\n",
       "Text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the dataset has any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba2b0c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         11045\n",
       "sadness      6722\n",
       "fear         5410\n",
       "anger        4297\n",
       "surprise     4062\n",
       "neutral      2254\n",
       "disgust       856\n",
       "shame         146\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check the value_count for each emotion - it will give each emotion is containing this much amt of text \n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd743f",
   "metadata": {},
   "source": [
    "Pre-Processing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814dc78",
   "metadata": {},
   "source": [
    "To clean the text messages by removing stopwords, numbers, and punctuation. Then convert each word into its base form by using the lemmatization process in the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "889be5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "stop_words =  stopwords.words('english')\n",
    "def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):\n",
    "    # Clean the text, with the option to remove stop_words and to lemmatize word\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text =  re.sub(r'http\\S+',' link ', text)\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text) # remove numbers\n",
    "        \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if lemmatize_words:\n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da51f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text\n",
    "df[\"cleaned_text\"] = df[\"Text\"].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a8f1742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "      <td>Why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade list tommorow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL MAN I H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>Such eye The true hazel eye brilliant Regular ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>Iluvmiasantos ugh babe hugggzzz u babe naamaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34787</th>\n",
       "      <td>surprise</td>\n",
       "      <td>@MichelGW have you gift! Hope you like it! It'...</td>\n",
       "      <td>MichelGW gift Hope like It hand made wear It k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34788</th>\n",
       "      <td>joy</td>\n",
       "      <td>The world didnt give it to me..so the world MO...</td>\n",
       "      <td>The world didnt give world MOST DEFINITELY cnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34789</th>\n",
       "      <td>anger</td>\n",
       "      <td>A man robbed me today .</td>\n",
       "      <td>A man robbed today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34790</th>\n",
       "      <td>fear</td>\n",
       "      <td>Youu call it JEALOUSY, I call it of #Losing YO...</td>\n",
       "      <td>Youu call JEALOUSY I call Losing YOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34791</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I think about you baby, and I dream about you ...</td>\n",
       "      <td>I think baby I dream time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34792 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                               Text  \\\n",
       "0       neutral                                             Why ?    \n",
       "1           joy    Sage Act upgrade on my to do list for tommorow.   \n",
       "2       sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3           joy   Such an eye ! The true hazel eye-and so brill...   \n",
       "4           joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "...         ...                                                ...   \n",
       "34787  surprise  @MichelGW have you gift! Hope you like it! It'...   \n",
       "34788       joy  The world didnt give it to me..so the world MO...   \n",
       "34789     anger                           A man robbed me today .    \n",
       "34790      fear  Youu call it JEALOUSY, I call it of #Losing YO...   \n",
       "34791   sadness  I think about you baby, and I dream about you ...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0                                                    Why  \n",
       "1                         Sage Act upgrade list tommorow  \n",
       "2      ON THE WAY TO MY HOMEGIRL BABY FUNERAL MAN I H...  \n",
       "3      Such eye The true hazel eye brilliant Regular ...  \n",
       "4      Iluvmiasantos ugh babe hugggzzz u babe naamaze...  \n",
       "...                                                  ...  \n",
       "34787  MichelGW gift Hope like It hand made wear It k...  \n",
       "34788  The world didnt give world MOST DEFINITELY cnt...  \n",
       "34789                                 A man robbed today  \n",
       "34790               Youu call JEALOUSY I call Losing YOU  \n",
       "34791                          I think baby I dream time  \n",
       "\n",
       "[34792 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get clean dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015ddd7",
   "metadata": {},
   "source": [
    "Model features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "caca7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X-features,y-labels\n",
    "X = df['cleaned_text']\n",
    "y = df['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96023124",
   "metadata": {},
   "source": [
    "Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8041f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b80dc5",
   "metadata": {},
   "source": [
    "Pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4f951e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model in pipeline(LogisticRegression Pipeline)\n",
    "pipe_lr = Pipeline(steps=[\n",
    "                               ('pre_processing',TfidfVectorizer(lowercase=False)),\n",
    "                                 ('lr',LogisticRegression())\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af9a385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneka\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre_processing', TfidfVectorizer(lowercase=False)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "pipe_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02b538e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre_processing', TfidfVectorizer(lowercase=False)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c885d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prediction from the test set\n",
    "y_test = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc73a134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Accuracy\n",
    "pipe_lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26b59d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Prediction\n",
    "sample_text = \"@llumiasantos ugh babe.. hugggzzz for u.! babe naamazed nga ako e ababe e,despite nega's mas pinaramdam at fil lo ang\"\n",
    "pipe_lr.predict([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb38e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneka\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6e3a3",
   "metadata": {},
   "source": [
    "save model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "291dd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"EmotionDetection/pipe_lr.pkl\",\"wb\")\n",
    "pickle.dump(pipe_lr,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
